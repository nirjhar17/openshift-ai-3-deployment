<!DOCTYPE html>
<html>
<head>
    <title>From Container to Production: Deploying LLMs on OpenShift AI 3.0</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; line-height: 1.6; }
        h1 { font-size: 2.5em; margin-bottom: 0.5em; }
        h2 { font-size: 1.8em; margin-top: 2em; border-bottom: 1px solid #eee; padding-bottom: 0.3em; }
        h3 { font-size: 1.4em; margin-top: 1.5em; }
        code { background: #f4f4f4; padding: 2px 6px; border-radius: 3px; font-family: 'Monaco', 'Menlo', monospace; }
        pre { background: #f4f4f4; padding: 16px; border-radius: 6px; overflow-x: auto; }
        pre code { background: none; padding: 0; }
        blockquote { border-left: 4px solid #ddd; margin: 0; padding-left: 16px; color: #666; }
        ul, ol { padding-left: 24px; }
        a { color: #0366d6; }
        strong { font-weight: 600; }
        em { font-style: italic; }
        hr { border: none; border-top: 1px solid #eee; margin: 2em 0; }
    </style>
</head>
<body>
<h1>From Container to Production: Deploying LLMs on OpenShift AI 3.0 with Intelligent Load Balancing</h1>

<p><em>A complete guide to deploying Large Language Models using OpenShift AI 3.0's new disaggregated inference architecture</em></p>

<hr>

<h2>Introduction</h2>

<p><strong>Red Hat recently released OpenShift AI 3.0</strong>, introducing a new way to deploy and serve Large Language Models at scale. In earlier versions, deploying a model meant creating an <code>InferenceService</code>, exposing it via OpenShift Routes, and handling authentication separately. It worked, but scaling across multiple GPUs required manual load balancing, and securing endpoints needed extra configuration.</p>

<p>OpenShift AI 3.0 changes this with <strong>llm-d</strong> — a disaggregated inference architecture that intelligently distributes requests across multiple GPU pods. Instead of traditional Routes, we now use the <strong>Kubernetes Gateway API</strong> for external access, which provides built-in TLS termination and seamless integration with <strong>Red Hat Connectivity Link</strong> (Kuadrant) for authentication and rate limiting. The new <code>LLMInferenceService</code> resource handles everything — it automatically creates an <strong>EPP Scheduler</strong> that routes requests to the best available GPU based on queue length, KV cache utilization, and prefix matching.</p>

<p>In this blog, I'll walk you through deploying an LLM on OpenShift AI 3.0 from scratch — setting up the Gateway for secure HTTPS access, enabling authentication, and handling real-world GPU compatibility issues with Tesla T4. By the end, you'll have a production-ready model endpoint with automatic TLS, intelligent load balancing, and enterprise-grade security.</p>

<p><strong>What we'll cover:</strong></p>
<ul>
<li>Setting up the required operators</li>
<li>Configuring Gateway API for external access</li>
<li>Enabling authentication with Kuadrant</li>
<li>Handling TLS certificates</li>
<li>Overcoming GPU compatibility challenges (Tesla T4)</li>
</ul>

<p><strong>GitHub Repository:</strong> All configuration files are available at <a href="https://github.com/nirjhar17/openshift-ai-3-deployment">github.com/nirjhar17/openshift-ai-3-deployment</a></p>

<hr>

<h2>Architecture Overview</h2>

<p>Before diving into the deployment, let's understand what we're building:</p>

<p><strong>The Request Flow:</strong></p>
<ol>
<li><strong>External Traffic</strong> → User sends HTTPS request</li>
<li><strong>Gateway API</strong> → Receives traffic on port 443, terminates TLS, routes to services</li>
<li><strong>Kuadrant</strong> → Authorino validates tokens, Limitador enforces rate limits</li>
<li><strong>EPP Scheduler</strong> → Intelligently routes to the best GPU pod based on queue length, KV cache, and prefix matching</li>
<li><strong>vLLM Pods</strong> → Multiple GPU pods process requests in parallel</li>
</ol>

<p><strong>Components:</strong></p>
<ul>
<li><strong>Gateway API (openshift-ai-inference)</strong>: Receives external traffic on port 443 (HTTPS), handles TLS termination using OpenShift-managed certificate, routes requests to appropriate services</li>
<li><strong>Kuadrant (Red Hat Connectivity Link)</strong>: Authorino handles authentication (token validation), Limitador handles rate limiting, AuthPolicy defines who can access the model</li>
<li><strong>EPP Scheduler (Endpoint Picker Protocol)</strong>: Intelligent request routing across multiple GPU pods using Queue Scorer, KV Cache Scorer, and Prefix Cache Scorer</li>
<li><strong>vLLM Pods</strong>: Multiple GPU nodes running the Qwen3-0.6B model</li>
</ul>

<h3>What is LLM-D?</h3>

<p>Traditional LLM serving runs the entire inference (prefill + decode) on a single GPU. LLM-D allows:</p>
<ol>
<li><strong>Multiple Replicas</strong>: Run multiple vLLM pods, each with its own GPU</li>
<li><strong>Intelligent Routing</strong>: EPP Scheduler decides which pod handles each request</li>
<li><strong>Better Utilization</strong>: Requests are distributed based on current load and cache state</li>
</ol>

<hr>

<h2>Prerequisites</h2>

<h3>Cluster Requirements</h3>
<ul>
<li>OpenShift 4.19+ cluster</li>
<li>GPU nodes with NVIDIA GPUs</li>
<li>Cluster admin access</li>
</ul>

<h3>Required Operators</h3>
<p>Install these operators from <strong>OperatorHub</strong> in the OpenShift Web Console:</p>
<ul>
<li><strong>Red Hat OpenShift AI</strong> (Channel: fast-3.x) — Core AI/ML platform with KServe</li>
<li><strong>NVIDIA GPU Operator</strong> (Channel: v25.10) — GPU device plugin & drivers</li>
<li><strong>Red Hat Connectivity Link</strong> (Channel: stable) — API gateway, auth & rate limiting</li>
</ul>

<h4>Installation Steps (OpenShift Console)</h4>
<ol>
<li>Navigate to <strong>Operators → OperatorHub</strong></li>
<li>Search for each operator by name</li>
<li>Click <strong>Install</strong> and select the appropriate channel</li>
<li>Wait for the operator to show <strong>Succeeded</strong> status</li>
</ol>

<hr>

<h2>Step 1: Configure DataScienceCluster</h2>
<p>The DataScienceCluster resource configures which OpenShift AI components are enabled.</p>
<pre><code>apiVersion: datasciencecluster.opendatahub.io/v1
kind: DataScienceCluster
metadata:
  name: default-dsc
spec:
  components:
    kserve:
      managementState: Managed
    dashboard:
      managementState: Managed
    llamastackoperator:
      managementState: Managed</code></pre>

<hr>

<h2>Step 2: Set Up Gateway API</h2>
<p>Gateway API is the modern replacement for Ingress, providing more powerful routing capabilities.</p>

<h3>Create GatewayClass</h3>
<pre><code>apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: openshift-ai-inference
spec:
  controllerName: openshift.io/gateway-controller/v1</code></pre>

<h3>Create Gateway with TLS</h3>
<pre><code>apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: openshift-ai-inference
  namespace: openshift-ingress
spec:
  gatewayClassName: openshift-ai-inference
  listeners:
    - name: https
      port: 443
      protocol: HTTPS
      tls:
        mode: Terminate
        certificateRefs:
          - kind: Secret
            name: default-gateway-tls</code></pre>

<hr>

<h2>Step 3: Enable Kuadrant Authentication</h2>
<pre><code>apiVersion: kuadrant.io/v1beta1
kind: Kuadrant
metadata:
  name: kuadrant
  namespace: kuadrant-system
spec: {}</code></pre>

<h3>Enable Authorino TLS (Important!)</h3>
<pre><code>oc annotate svc authorino-authorino-authorization -n kuadrant-system \
  service.beta.openshift.io/serving-cert-secret-name=authorino-tls</code></pre>

<hr>

<h2>Step 4: Create Hardware Profile</h2>
<pre><code>apiVersion: infrastructure.opendatahub.io/v1
kind: HardwareProfile
metadata:
  name: gpu-profile
  namespace: redhat-ods-applications
spec:
  identifiers:
    - identifier: nvidia.com/gpu
      displayName: GPU
      defaultCount: 1
      resourceType: Accelerator</code></pre>

<hr>

<h2>Step 5: Deploy the LLM with LLM-D</h2>
<pre><code>apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: qwen3-0-6b
  namespace: my-first-model
  annotations:
    security.opendatahub.io/enable-auth: "false"
spec:
  replicas: 2
  model:
    uri: hf://Qwen/Qwen3-0.6B
    name: Qwen/Qwen3-0.6B
  template:
    containers:
      - name: main
        image: vllm/vllm-openai:v0.8.4
        env:
          - name: VLLM_USE_V1
            value: "0"
          - name: VLLM_ADDITIONAL_ARGS
            value: "--dtype=half --enforce-eager"</code></pre>

<hr>

<h2>GPU Compatibility: Tesla T4 Challenges</h2>

<h3>Challenge 1: vLLM V1 Engine Incompatibility</h3>
<p><strong>Error:</strong> RuntimeError: Cannot use FA version 2</p>
<p><strong>Solution:</strong> Set <code>VLLM_USE_V1=0</code></p>

<h3>Challenge 2: bfloat16 Not Supported</h3>
<p><strong>Error:</strong> Bfloat16 requires compute capability 8.0+</p>
<p><strong>Solution:</strong> Use <code>--dtype=half</code></p>

<h3>Challenge 3: Slow Startup</h3>
<p><strong>Solution:</strong> Use <code>--enforce-eager</code></p>

<h3>Challenge 4: Red Hat vLLM Image Incompatibility</h3>
<p><strong>Solution:</strong> Use upstream image <code>vllm/vllm-openai:v0.8.4</code></p>

<hr>

<h2>Step 6: Verify Deployment</h2>
<pre><code>oc get llminferenceservice -n my-first-model
oc get pods -n my-first-model</code></pre>

<h3>Test the Endpoint</h3>
<pre><code>curl -sk https://GATEWAY_URL/my-first-model/qwen3-0-6b/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "Qwen/Qwen3-0.6B", "messages": [{"role": "user", "content": "Hello"}]}'</code></pre>

<hr>

<h2>Conclusion</h2>
<p>We successfully deployed an LLM on OpenShift AI 3.0 using the new LLM-D architecture. Key takeaways:</p>
<ol>
<li><strong>LLM-D enables intelligent load balancing</strong> across multiple GPU pods</li>
<li><strong>Gateway API with TLS</strong> provides secure external access</li>
<li><strong>Kuadrant handles authentication</strong> with flexible policies</li>
<li><strong>GPU compatibility matters</strong> - Tesla T4 requires specific vLLM settings</li>
</ol>

<h3>What's Next?</h3>
<p>In the next blog, we'll explore rate limiting with RateLimitPolicy and tiered access.</p>

<hr>

<h2>Resources</h2>
<ul>
<li><a href="https://github.com/nirjhar17/openshift-ai-3-deployment">GitHub Repository</a></li>
<li><a href="https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0">OpenShift AI 3.0 Documentation</a></li>
<li><a href="https://kuadrant.io/">Kuadrant Documentation</a></li>
</ul>

<p><em>Author: Nirjhar Jajodia | December 2025</em></p>

</body>
</html>
